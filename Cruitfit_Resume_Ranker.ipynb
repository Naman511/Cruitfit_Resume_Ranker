{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CVc3KYVN2Tfa"},"outputs":[],"source":["# Ensure necessary libraries are installed:\n","!pip install spacy pymupdf matplotlib squarify numpy\n","!pip install language-tool-python\n","!pip install nltk textstat\n","# python -m spacy download en_core_web_sm"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"elapsed":9343,"status":"error","timestamp":1750700042116,"user":{"displayName":"Naman Pandey","userId":"18400928208288391728"},"user_tz":-330},"id":"DED8aY6G10bd","outputId":"d6d64ea9-19eb-4d46-ac71-e77a2986227b"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'squarify'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-2642607174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msquarify\u001b[0m \u001b[0;31m# for treemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# PyMuPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'squarify'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import spacy\n","from spacy.matcher import Matcher\n","from collections import Counter\n","import re\n","import matplotlib.pyplot as plt\n","import squarify # for treemap\n","import numpy as np\n","import fitz  # PyMuPDF\n","import math\n","import os # interact with the operating system. It is a standard library module, meaning it is included with Python and does not need to be installed separately.\n","import statistics\n","import language_tool_python\n","import nltk\n","from textstat import flesch_reading_ease\n","# --- Configuration ---\n","\n","# Load spaCy model\n","try:\n","    nlp = spacy.load(\"en_core_web_sm\")\n","except OSError:\n","    print(\"spaCy model 'en_core_web_sm' not found. Please run: python -m spacy download en_core_web_sm\")\n","    exit()\n","\n","KNOWN_SKILLS = [\n","    # Programming Languages\n","    \"python\", \"java\", \"javascript\", \"c++\", \"c#\", \"php\", \"ruby\", \"swift\", \"kotlin\", \"scala\", \"go\", \"rust\", \"typescript\",\n","    \"sql\", \"nosql\", \"r\", \"matlab\", \"perl\", \"shell scripting\", \"bash\",\n","    # Web Development\n","    \"html\", \"css\", \"react\", \"react.js\", \"angular\", \"vue\", \"vue.js\", \"next.js\", \"nuxtjs\", \"nodejs\", \"node.js\", \"express\", \"express.js\",\n","    \"django\", \"flask\", \"rubyonrails\", \"asp.net\", \"jquery\", \"bootstrap\", \"tailwind css\", \"sass\", \"less\",\n","    \"webpack\", \"babel\", \"rest\", \"restful apis\", \"graphql\", \"soap\", \"ajax\", \"json\", \"xml\", \"yaml\",\n","    # Mobile Development\n","    \"android development\", \"ios development\", \"swiftui\", \"objective-c\", \"flutter\", \"react native\", \"xamarin\", \"kotlin multiplatform\",\n","    # Databases\n","    \"mysql\", \"postgresql\", \"mongodb\", \"oracle database\", \"sql server\", \"sqlite\", \"redis\", \"memcached\", \"cassandra\", \"elasticsearch\",\n","    \"dynamodb\", \"firebase\",\n","    # Cloud & DevOps\n","    \"aws\", \"amazon web services\", \"azure\", \"microsoft azure\", \"gcp\", \"google cloud platform\", \"docker\", \"kubernetes\", \"k8s\",\n","    \"jenkins\", \"gitlab ci\", \"github actions\", \"ansible\", \"terraform\", \"puppet\", \"chef\", \"ci/cd\", \"continuous integration\", \"continuous deployment\",\n","    \"linux\", \"unix\", \"server administration\", \"powershell\", \"serverless\", \"microservices\", \"nginx\", \"apache\", \"load balancing\",\n","    # Data Science & ML/AI\n","    \"machine learning\", \"deep learning\", \"nlp\", \"natural language processing\", \"computer vision\", \"data analysis\",\n","    \"data mining\", \"data visualization\", \"pandas\", \"numpy\", \"scipy\", \"scikit-learn\", \"sklearn\", \"tensorflow\", \"keras\", \"pytorch\",\n","    \"apache spark\", \"hadoop\", \"tableau\", \"power bi\", \"statistics\", \"big data\", \"etl\", \"data warehousing\",\n","    # Software Engineering & Architecture\n","    \"agile\", \"scrum\", \"kanban\", \"waterfall\", \"design patterns\", \"data structures\", \"object-oriented programming\", \"oop\",\n","    \"functional programming\", \"system design\", \"software architecture\", \"software development life cycle\", \"sdlc\",\n","    \"unit testing\", \"integration testing\", \"end-to-end testing\", \"qa\", \"quality assurance\", \"selenium\", \"cypress\", \"jest\", \"junit\", \"pytest\", \"tdd\", \"bdd\",\n","    # Cybersecurity\n","    \"cybersecurity\", \"information security\", \"penetration testing\", \"ethical hacking\", \"network security\", \"cryptography\", \"siem\", \"soc\",\n","    # Business & Soft Skills\n","    \"project management\", \"product management\", \"communication\", \"teamwork\", \"leadership\", \"problem solving\", \"analytical skills\",\n","    \"customer service\", \"technical writing\", \"ui/ux design\", \"user interface\", \"user experience\", \"figma\", \"adobe xd\", \"sketch\",\n","    # Other Technical Skills\n","    \"api design\", \"sdk development\", \"blockchain\", \"game development\", \"unity\", \"unreal engine\", \"virtual reality\", \"vr\", \"augmented reality\", \"ar\",\n","    \"embedded systems\", \"iot\", \"robotics\", \"photoshop\", \"illustrator\", \"autocad\", \"blender\",\n","    # Marketing\n","    \"digital marketing\", \"social media marketing\", \"content marketing\", \"email marketing\", \"paid advertising\", \"ppc\", \"search engine marketing\", \"sem\",\n","    \"marketing automation\", \"crm\", \"customer relationship management\", \"market research\", \"marketing strategy\", \"brand management\", \"public relations\", \"affiliate marketing\",\n","    # SEO\n","    \"search engine optimization\", \"seo\", \"seo strategy\", \"keyword research\", \"on-page seo\", \"off-page seo\", \"technical seo\", \"local seo\", \"seo analytics\",\n","    # Branding\n","    \"brand strategy\", \"brand identity\", \"brand messaging\", \"brand development\", \"brand awareness\", \"brand positioning\", \"brand communication\",\n","    # Finance\n","    \"financial analysis\", \"financial modeling\", \"budgeting\", \"forecasting\", \"accounting\", \"corporate finance\", \"investment management\", \"financial reporting\",\n","    # Strategy\n","    \"business strategy\", \"strategic planning\", \"market analysis\", \"competitive analysis\", \"growth strategy\", \"innovation strategy\", \"business development\",\n","    # Sales\n","    \"sales management\", \"sales strategy\", \"lead generation\", \"sales process\", \"account management\", \"negotiation\", \"sales presentations\",\n","    \"customer acquisition\", \"sales forecasting\", \"channel sales\", \"key account management\", \"solution selling\", \"sales operations\",\n","    # Post-MBA Fields\n","    \"management consulting\", \"operations management\", \"supply chain management\", \"human resources\", \"organizational development\",\n","    \"entrepreneurship\", \"venture capital\", \"private equity\", \"investment banking\", \"mergers and acquisitions\", \"real estate\",\n","    \"healthcare management\", \"technology management\", \"international business\", \"corporate social responsibility\", \"sustainability\"\n","]\n","KNOWN_SKILLS = sorted(list(set(skill.lower() for skill in KNOWN_SKILLS)))\n","\n","\n","# Keywords for Job Description Skill Importance\n","IMPORTANCE_KEYWORDS = {\n","    # High importance (3.0 - 2.5): Non-negotiable or critical skills\n","    \"must\": 3.0,\n","    \"required\": 3.0,\n","    \"essential\": 3.0,\n","    \"critical\": 3.0,\n","    \"mandatory\": 3.0,\n","    \"vital\": 3.0,\n","    \"expert\": 2.8,\n","    \"proficient\": 2.5,\n","\n","    # Moderate importance (2.0 - 1.1): Important but not always mandatory\n","    \"strong\": 2.0,\n","    \"demonstrated\": 1.8,\n","    \"experienced\": 1.6,\n","    \"preferred\": 1.5,\n","    \"desired\": 1.5,\n","    \"highly valued\": 1.5,\n","    \"significant\": 1.3,\n","    \"recommended\": 1.1,\n","\n","    # Low importance (1.0 - 0.1): Nice-to-have or supplementary skills\n","    \"familiar\": 0.8,\n","    \"knowledge of\": 0.6,\n","    \"basic\": 0.5,\n","    \"exposure to\": 0.4,\n","    \"awareness of\": 0.3,\n","    \"develop\": 0.5,\n","    \"design\": 0.5,\n","    \"implement\": 0.5,\n","    \"work with\": 0.4,\n","    \"understanding of\": 0.3,\n","    \"introductory\": 0.2,\n","    \"optional\": 0.1\n","}\n","\n","# Keywords for Resume Skill Proficiency\n","PROFICIENCY_MODIFIERS = {\n","    # High proficiency (3.0 - 2.5): Expert or near-expert level\n","    \"expert in\": 3.0,\n","    \"mastery\": 3.0,\n","    \"specialized in\": 2.8,\n","    \"advanced\": 2.8,\n","    \"proficient in\": 2.5,\n","    \"highly skilled\": 2.5,\n","\n","    # Moderate proficiency (2.0 - 1.1): Competent to strong skills\n","    \"strong experience\": 2.0,\n","    \"lead\": 2.0,\n","    \"managed\": 1.8,\n","    \"developed\": 1.8,\n","    \"experienced in\": 1.6,\n","    \"skilled in\": 1.5,\n","    \"competent in\": 1.5,\n","    \"strong\": 1.5,\n","    \"certified in\": 1.4,\n","    \"trained in\": 1.3,\n","    \"practiced in\": 1.2,\n","    \"experience with\": 1.1,\n","\n","    # Low proficiency (1.0 - 0.1): Basic or limited skills\n","    \"worked with\": 1.0,\n","    \"used\": 0.9,\n","    \"familiar with\": 0.8,\n","    \"knowledge of\": 0.7,\n","    \"exposure to\": 0.6,\n","    \"basic understanding of\": 0.5,\n","    \"introductory\": 0.4,\n","    \"beginner\": 0.3,\n","    \"learning\": 0.2,\n","    \"aware of\": 0.1\n","}\n","\n","\n","# Unified Degrees List\n","DEGREES_S1 = [\n","    \"bachelor\", \"bachelors\", \"b.sc\", \"bsc\", \"b.e\", \"be\", \"b.tech\", \"btech\",\n","    \"master\", \"masters\", \"m.sc\", \"msc\", \"m.e\", \"me\", \"m.tech\", \"mtech\",\n","    \"mba\", \"phd\", \"doctorate\", \"associate\", \"diploma\"\n","]\n","DEGREES_S2 = [\n","    \"bachelor\", \"b.tech\", \"btech\", \"b.e\", \"b.sc\", \"ba\", \"bca\",\n","    \"master\", \"m.tech\", \"mtech\", \"m.e\", \"m.sc\", \"ma\", \"mca\",\n","    \"phd\", \"doctorate\", \"mba\", \"pgdm\", \"diploma\"\n","]\n","COMBINED_DEGREES = sorted(list(set([d.lower() for d in DEGREES_S1 + DEGREES_S2])))\n","\n","# Degree Hierarchy: Lower number is higher rank\n","DEGREE_HIERARCHY = {\n","    \"phd\": 1, \"doctorate\": 1,\n","    \"master\": 2, \"masters\": 2, \"m.tech\": 2, \"mtech\": 2, \"m.e\": 2, \"me\": 2, \"m.sc\": 2, \"msc\": 2, \"ma\": 2, \"mca\": 2, \"mba\": 2, \"pgdm\": 2,\n","    \"bachelor\": 3, \"bachelors\": 3, \"b.tech\": 3, \"btech\": 3, \"b.e\": 3, \"be\": 3, \"b.sc\": 3, \"bsc\": 3, \"ba\": 3, \"bca\": 3,\n","    \"diploma\": 4,\n","    \"associate\": 5\n","}\n","DEFAULT_DEGREE_RANK = 10 # For degrees not in hierarchy\n","\n","# Regex for experience\n","EXPERIENCE_REGEX = re.compile(r'(?:(?:minimum|at least)\\s*)?(\\d{1,2})\\+?\\s*(?:\\+?\\s*)?(?:years?|yrs?)\\s*(?:of)?\\s*(?:experience|exp)?', re.IGNORECASE)\n","\n","# SWOT Analysis Thresholds\n","STRENGTH_PROFICIENCY_THRESHOLD = 70 #minimum proficiency that a person should have in skill according to resume to qualify it as his strength\n","STRENGTH_IMPORTANCE_THRESHOLD = 60\n","WEAKNESS_IMPORTANCE_THRESHOLD = 60\n","WEAKNESS_PROFICIENCY_THRESHOLD = 30\n","OPPORTUNITY_PROFICIENCY_THRESHOLD = 50\n","OPPORTUNITY_IMPORTANCE_THRESHOLD = 40 # Lower importance for this specific JD\n","THREAT_JD_IMPORTANCE_THRESHOLD = 60 # Critical skill for JD\n","THREAT_PROFICIENCY_THRESHOLD = 10   # Very low or missing proficiency in critical skill\n","\n","\n","# --- Helper Functions ---\n","def preprocess_text(text: str) -> str:\n","    \"\"\"Cleans and normalizes text.\"\"\"\n","    text = text.lower() #lowers all the text\n","    text = re.sub(r'\\s+', ' ', text) #Replaces multiple whitespace characters with a single space.\n","    text = re.sub(r'[^\\w\\s.\\-/#+]', '', text)\n","    return text.strip()\n","\n","def build_skill_matcher(nlp_vocab, skill_list: list) -> Matcher:\n","    \"\"\"Builds a spaCy Matcher for the given skill list.\"\"\"\n","    matcher = Matcher(nlp_vocab)\n","    for skill in skill_list:\n","        pattern = [{\"LOWER\": token} for token in skill.lower().split()]\n","        matcher.add(skill.upper(), [pattern])\n","    return matcher\n","\n","def extract_text_from_pdf(pdf_path: str) -> str:\n","    \"\"\"Extracts all text from a PDF file.\"\"\"\n","    try:\n","        doc = fitz.open(pdf_path)\n","        text = \"\"\n","        for page_num in range(len(doc)):\n","            page = doc.load_page(page_num)\n","            text += page.get_text(\"text\")\n","        return text.lower()\n","    except Exception as e:\n","        print(f\"Error reading PDF {pdf_path}: {e}\")\n","        return \"\"\n","\n","def extract_degrees_from_text(text: str, degree_list: list) -> list:\n","    \"\"\"Extracts recognized degrees from text.\"\"\"\n","    found_degrees = set()\n","    processed_text = text.lower()\n","    for degree_pattern in degree_list:\n","        if re.search(r'\\b' + re.escape(degree_pattern) + r'\\b', processed_text):\n","            found_degrees.add(degree_pattern)\n","    return sorted(list(found_degrees))\n","\n","def calculate_max_possible_absolute_score(jd_skill_importance_map: dict) -> float:\n","    \"\"\"\n","    Calculates the maximum possible absolute score a candidate could achieve.\n","    Assumes proficiency of 100 for all JD skills and a high median frequency.\n","    \"\"\"\n","    if not jd_skill_importance_map:\n","        return 0.0\n","\n","    # Sum of combined skill scores assuming max proficiency (100)\n","    max_skill_sum = 0.0\n","    for skill, data in jd_skill_importance_map.items():\n","        importance_score = data.get(\"score\", 0)\n","        max_skill_sum += importance_score\n","    max_absolute_score = max_skill_sum\n","    return max_absolute_score\n","# --- Job Description Analysis Module ---\n","def extract_skills_with_context_jd(doc: spacy.tokens.Doc, skill_list: list, matcher: Matcher) -> dict:\n","    \"\"\"Extracts skills and their sentence contexts from a spaCy Doc.\"\"\"\n","    skill_occurrences = {}\n","    matches = matcher(doc)\n","    for match_id, start, end in matches:\n","        original_skill_name = nlp.vocab.strings[match_id].lower()\n","        if original_skill_name in skill_list:\n","            sentence = doc[start:end].sent\n","            skill_occurrences.setdefault(original_skill_name, []).append(sentence)\n","    return skill_occurrences\n","\n","def rate_skills_from_job_description(job_description_text: str) -> tuple[dict, str | None, list]:\n","    \"\"\"\n","    Analyzes a job description to rate skills by importance.\n","    Returns a dictionary of {skill: {\"score\": importance_score (0-100), \"frequency\": N}},\n","    required experience, and required degrees.\n","    \"\"\"\n","    cleaned_text = preprocess_text(job_description_text)\n","    doc = nlp(cleaned_text)\n","\n","    skill_matcher = build_skill_matcher(nlp.vocab, KNOWN_SKILLS)\n","    skill_sentence_map = extract_skills_with_context_jd(doc, KNOWN_SKILLS, skill_matcher)\n","\n","    if not skill_sentence_map:\n","        print(\"No known skills found in the job description.\")\n","        return {}, None, []\n","\n","    skill_raw_scores = Counter()\n","    for skill, sentences in skill_sentence_map.items():\n","        frequency = len(sentences)\n","        base_score = frequency\n","        keyword_score_boost = 0\n","        for sentence in sentences:\n","            sentence_text = sentence.text.lower()\n","            for keyword, weight in IMPORTANCE_KEYWORDS.items():\n","                if re.search(r'\\b' + re.escape(keyword) + r'\\b', sentence_text):\n","                    keyword_score_boost += weight\n","        total_score = base_score + keyword_score_boost\n","        skill_raw_scores[skill] = total_score\n","\n","    max_raw_score = float(max(skill_raw_scores.values(), default=1))\n","    if max_raw_score == 0: max_raw_score = 1\n","\n","    rated_skills_importance = {}\n","    for skill, raw_score in skill_raw_scores.items():\n","        normalized_score = round((np.log1p(raw_score) / np.log1p(max_raw_score) * 100))\n","        rated_skills_importance[skill] = {\"score\": normalized_score, \"frequency\": len(skill_sentence_map[skill])}\n","\n","    jd_degrees_found = extract_degrees_from_text(cleaned_text, COMBINED_DEGREES)\n","\n","    return rated_skills_importance,jd_degrees_found\n","\n","def generate_jd_treemap(skills_dict_with_freq: dict):\n","    \"\"\"Generates and saves a treemap of JD skill importance.\"\"\"\n","    if not skills_dict_with_freq:\n","        print(\"No skills to plot for treemap.\")\n","        return\n","\n","    labels = [f\"{skill.capitalize()}\\n{data['score']}\" for skill, data in skills_dict_with_freq.items() if data['score'] > 0]\n","    sizes = [data['score'] for data in skills_dict_with_freq.values() if data['score'] > 0]\n","\n","    if not labels:\n","        print(\"All skill scores are 0, treemap will not be generated.\")\n","        return\n","\n","    colors = plt.cm.viridis(np.linspace(0, 1, len(sizes)))\n","    fig = plt.figure(figsize=(14, 9))\n","    squarify.plot(sizes=sizes, label=labels, color=colors, alpha=0.7, text_kwargs={'fontsize': 10, 'wrap': True})\n","    plt.title('Job Description: Skill Importance Treemap (0-100 Scale)', fontsize=16)\n","    plt.axis('off')\n","    try:\n","        plt.savefig('jd_skill_treemap.png')\n","        print(\"\\nJob Description skill treemap saved as 'jd_skill_treemap.png'\")\n","    except Exception as e:\n","        print(f\"Error saving treemap: {e}\")\n","    finally:\n","        plt.close(fig)\n","\n","# --- Resume Analysis Module ---\n","def identify_skills_and_frequency_resume(text: str, skill_list: list) -> Counter:\n","    \"\"\"Identifies skills in resume text and counts their frequency using regex.\"\"\"\n","    found_skills_freq = Counter()\n","    processed_text = text.lower()\n","    for skill in skill_list:\n","        pattern = r\"\\b\" + re.escape(skill.lower()) + r\"\\b\"\n","        try:\n","            matches = re.findall(pattern, processed_text)\n","            if matches:\n","                found_skills_freq[skill] = len(matches)\n","        except re.error as e:\n","            print(f\"Regex error for skill '{skill}': {e}\")\n","    return found_skills_freq\n","\n","def calculate_skill_weights_resume(text: str, identified_skills_freq: Counter, proficiency_modifiers_dict: dict) -> dict:\n","    \"\"\"Calculates raw weighted scores for skills based on sentence context and frequency in resume.\"\"\"\n","    weighted_skills = {}\n","    processed_text = text.lower()\n","    # Split into sentences by punctuation\n","    sentences = re.split(r'(?<=[\\.\\!\\?])\\s+', processed_text)\n","\n","    for skill, frequency in identified_skills_freq.items():\n","        if frequency == 0:\n","            continue\n","        frequency_component = frequency\n","        modifier_sum = 0\n","        # Look for skill in each sentence\n","        for sentence in sentences:\n","            if re.search(r\"\\b\" + re.escape(skill.lower()) + r\"\\b\", sentence):\n","                for modifier_keyword, modifier_value in proficiency_modifiers_dict.items():\n","                    if re.search(r\"\\b\" + re.escape(modifier_keyword) + r\"\\b\", sentence):\n","                        modifier_sum += (modifier_value)\n","        raw_score = frequency_component + modifier_sum\n","        weighted_skills[skill] = raw_score\n","    return weighted_skills\n","\n","def rate_skills_resume_proficiency(weighted_skills: dict) -> dict:\n","    \"\"\"Normalizes resume skill scores to a 0-100 scale (proficiency rating).\"\"\"\n","    if not weighted_skills:\n","        return {}\n","    max_score = max(weighted_skills.values(), default=0)\n","    if max_score == 0:\n","        return {skill: 0.0 for skill in weighted_skills}\n","    return {skill: min(score,10) for skill, score in weighted_skills.items()}\n","\n","def analyze_resume_for_candidate(pdf_path: str, jd_skill_names: list) -> dict:\n","    \"\"\"\n","    Analyzes a single resume PDF.\n","    jd_skill_names: A list of skill names (strings) extracted from the job description.\n","    \"\"\"\n","    print(f\"\\nAnalyzing resume: {os.path.basename(pdf_path)}\")\n","    resume_text = extract_text_from_pdf(pdf_path)\n","    if not resume_text:\n","        return {\n","            \"filename\": os.path.basename(pdf_path), \"candidate_skills_proficiency\": {},\n","            \"degrees\": [], \"median_frequency\": 0.0, \"skill_frequencies\": Counter(),\n","            \"error\": f\"Could not extract text from {os.path.basename(pdf_path)}\"\n","        }\n","\n","    skills_freq_in_resume = identify_skills_and_frequency_resume(resume_text, KNOWN_SKILLS)\n","    weighted_resume_skills = calculate_skill_weights_resume(resume_text, skills_freq_in_resume, PROFICIENCY_MODIFIERS)\n","    candidate_skills_proficiency = rate_skills_resume_proficiency(weighted_resume_skills)\n","    degrees_in_resume = extract_degrees_from_text(resume_text, COMBINED_DEGREES)\n","\n","\n","    print(f\"  Found {len(candidate_skills_proficiency)} skills in resume.\")\n","    print(f\"  Degrees: {', '.join(degrees_in_resume) if degrees_in_resume else 'None'}\")\n","\n","    return {\n","        \"filename\": os.path.basename(pdf_path),\n","        \"candidate_skills_proficiency\": candidate_skills_proficiency,\n","        \"skill_frequencies\": skills_freq_in_resume, # Raw frequencies of skills found in resume\n","        \"degrees\": degrees_in_resume,\n","        \"error\": None\n","    }\n","\n","# --- EDUCATIONAL QUALIFICATION CHECK\n","def check_educational_qualification(jd_degrees: list, candidate_degrees: list) -> tuple[bool, str]:\n","    \"\"\"\n","    Checks if candidate meets minimum educational requirements from JD.\n","    Returns (is_qualified, message).\n","    \"\"\"\n","    if not jd_degrees:\n","        return True, \"No specific degree requirement in JD.\"\n","\n","    if not candidate_degrees:\n","        return False, \"Candidate has no listed degrees, but JD requires: \" + \", \".join(jd_degrees)\n","\n","    min_jd_rank_value = DEFAULT_DEGREE_RANK\n","    if jd_degrees:\n","        min_jd_rank_value = max(DEGREE_HIERARCHY.get(d, DEFAULT_DEGREE_RANK) for d in jd_degrees)\n","\n","    candidate_best_rank_value = DEFAULT_DEGREE_RANK\n","    if candidate_degrees:\n","         candidate_best_rank_value = min(DEGREE_HIERARCHY.get(d, DEFAULT_DEGREE_RANK) for d in candidate_degrees)\n","\n","    if candidate_best_rank_value < min_jd_rank_value:\n","        return True, \"Candidate's education exceeds the minimum requirement.\"\n","    elif candidate_best_rank_value == min_jd_rank_value:\n","        return True, \"Candidate's education meets the minimum requirement.\"\n","    else:\n","        return False, f\"Candidate's education does not meet the minimum requirement for {', '.join(jd_degrees)}.\"\n","\n","def calculate_ats_score(candidate_profile: dict, resume_text: str, pdf_path: str) -> float:\n","    \"\"\"\n","    Advanced ATS score calculation (0-100) using spaCy for NLP analysis.\n","    Evaluates resume's ATS-friendliness across multiple dimensions with enhanced accuracy.\n","    \"\"\"\n","    # Basic validation: Return 0.0 if there's an error in the profile or resume text is empty/None.\n","    if candidate_profile.get(\"error\") or not resume_text or not resume_text.strip():\n","        return 0.0\n","\n","    # Calculate scores for different components of the resume.\n","    # These components cover parsing, structure, content, formatting, and keywords.\n","    parsing_score = _calculate_parsing_compatibility(resume_text, pdf_path)\n","    structure_score = _calculate_structure_quality(resume_text)\n","    content_score = _calculate_content_optimization(resume_text, candidate_profile)\n","    formatting_score = _calculate_formatting_quality(resume_text)\n","    keyword_score = _calculate_keyword_density(resume_text, candidate_profile)\n","\n","    # Combine component scores using weights based on their typical importance for ATS.\n","    # Parsing compatibility is often the most critical first step.\n","    final_score = (\n","        parsing_score * 0.30 +      # Critical: Can the ATS even read the resume?\n","        structure_score * 0.25 +    # Important: Is the resume well-organized with standard sections?\n","        content_score * 0.20 +      # Important: Does the content showcase achievements and skills effectively?\n","        formatting_score * 0.15 +   # Relevant: Is the formatting clean, readable, and ATS-friendly?\n","        keyword_score * 0.10        # Relevant: Are appropriate keywords present?\n","    )\n","\n","    # Ensure the final score is within the 0-100 range and rounded to two decimal places.\n","    return round(min(max(final_score, 0.0), 100.0), 2)\n","\n","\n","def _calculate_parsing_compatibility(text: str, file_path: str) -> float:\n","    \"\"\"\n","    Scores the resume based on how easily an Applicant Tracking System (ATS) can parse its content.\n","    Penalizes for non-standard file formats, problematic characters, and poor text structure.\n","    \"\"\"\n","    score = 100.0 # Start with a perfect score and deduct points for issues.\n","\n","    # File format check: Penalize non-standard or problematic file types.\n","    if file_path:\n","        ext = os.path.splitext(file_path)[1].lower()\n","        if ext == '.pdf':\n","            # PDFs can sometimes be image-based or have complex encoding, making parsing harder.\n","            score -= 5  # Minor penalty.\n","        elif ext not in ['.docx', '.doc', '.txt']: # .txt is generally safest.\n","            score -= 30  # Heavier penalty for formats like .pages, .odt, image files, etc.\n","    else:\n","        # If no file path is provided, assume raw text which might miss some file-based context.\n","        score -= 5\n","\n","    # Character and pattern checks: Penalize characters and patterns known to cause issues for ATS.\n","    problematic_patterns = [\n","        (r'[^\\x00-\\x7F]', 3),      # Non-ASCII characters (e.g., unusual symbols, some foreign characters).\n","        (r'[\\u2010-\\u2015]', 2),  # Various dash types (hyphens are generally safer).\n","        (r'[\\u2018-\\u201F\\u201C\\u201D]', 2),  # Smart quotes (standard quotes ' \" are safer).\n","        (r'\\t{2,}', 4),           # Multiple consecutive tabs (often used for layout that breaks parsing).\n","        (r' {3,}', 3),            # Multiple consecutive spaces (use single spaces for separation).\n","        (r'[|⁄\\\\]{2,}', 7),       # Characters like multiple pipes or slashes used for tables/layout.\n","        (r'\\b(_{2,}|-{3,})\\b', 5) # Excessive underscores or long hyphens used as separators.\n","    ]\n","\n","    for pattern, penalty in problematic_patterns:\n","        matches = len(re.findall(pattern, text))\n","        if matches > 0:\n","            score -= min(matches * penalty, 20) # Cap penalty per pattern type to avoid excessive deduction.\n","\n","    # Check for text in ALL CAPS: Can be hard to read and is sometimes flagged by ATS.\n","    all_caps_words = sum(1 for word in text.split() if word.isupper() and len(word) > 1 and word.isalpha())\n","    total_words = len(text.split())\n","    if total_words > 0 and (all_caps_words / total_words) > 0.08: # If more than 8% of words are in ALL CAPS.\n","        score -= 10\n","\n","    # Check for overly long lines without breaks (can indicate poor formatting for parsing).\n","    lines = text.split('\\n')\n","    long_lines = sum(1 for line in lines if len(line) > 120) # Lines longer than 120 characters.\n","    if len(lines) > 0 and (long_lines / len(lines)) > 0.1: # If more than 10% of lines are very long.\n","        score -= 7\n","\n","    # spaCy-based analysis for text quality (if spaCy model 'nlp' is available).\n","    if 'nlp' in globals() and nlp:\n","        try:\n","            # Analyze a sample of the text for efficiency, but large enough for robustness.\n","            doc = nlp(text[:min(len(text), 3000)]) # Analyze up to the first 3000 characters.\n","\n","            # Check token count relative to text length.\n","            tokens = [token for token in doc if not token.is_space and not token.is_punct]\n","            if len(text) > 200 and len(tokens) < 50 :  # Very few useful tokens in a reasonably long text.\n","                score -= 25\n","            elif len(tokens) < 20 and len(text) > 100: # Extremely few tokens.\n","                 score -= 35\n","\n","            # Check sentence structure.\n","            sentences = list(doc.sents)\n","            if len(sentences) < 3 and len(tokens) > 70 : # Very few sentences for a decent amount of text.\n","                score -= 20\n","\n","            # Check average sentence length for extremes.\n","            if sentences:\n","                avg_sentence_len_tokens = sum(len([t for t in sent if not t.is_punct and not t.is_space]) for sent in sentences) / len(sentences)\n","                if avg_sentence_len_tokens > 35 : # Very long average sentence length can be hard to parse.\n","                    score -= 7\n","                if avg_sentence_len_tokens < 6 and len(sentences) > 10: # Many very short, possibly fragmented sentences.\n","                    score -= 7\n","        except Exception:\n","            # If spaCy processing fails, it might indicate issues with the text itself.\n","            score -= 15\n","    else:\n","        # Fallback checks if spaCy is not available.\n","        if len(text.split()) < 50 and len(text) > 300: # Basic word count check.\n","            score -= 20\n","        if len(re.findall(r'[.!?]\\s',text)) < 2 and len(text.split()) > 70 : # Approximate sentence count.\n","            score -=15\n","\n","    return max(score, 0.0)\n","\n","\n","def _calculate_structure_quality(text: str) -> float:\n","    \"\"\"\n","    Scores the resume based on its structural organization.\n","    Looks for essential sections, contact information, date ranges, and clear itemization (e.g., bullet points).\n","    \"\"\"\n","    score = 0.0 # Start with 0 and add points for positive structural elements.\n","    text_lower = text.lower() # Use lowercased text for case-insensitive matching.\n","\n","    # Define essential resume sections and their corresponding keywords and point values.\n","    essential_sections = {\n","        'contact': (r'\\b(contact|email|e-mail|phone|mobile|cell|linkedin|portfolio|github|address|location)\\b', 20),\n","        'summary': (r'\\b(summary|objective|profile|overview|about me|professional summary)\\b', 10),\n","        'experience': (r'\\b(experience|employment|work history|career|professional experience|positions held)\\b', 25),\n","        'education': (r'\\b(education|academic|degree|university|college|school|qualifications|certification)\\b', 20),\n","        'skills': (r'\\b(skills|technical skills|proficiencies|expertise|competencies|technologies|tools)\\b', 15)\n","    }\n","\n","    found_sections_count = 0\n","    for section_name, (pattern, points) in essential_sections.items():\n","        if re.search(pattern, text_lower):\n","            score += points\n","            found_sections_count += 1\n","\n","    # Penalize if fewer than 3 critical sections (Contact, Experience, Education) are clearly identifiable.\n","    # This check is simplified; a more robust check would ensure these specific three are present.\n","    if found_sections_count < 3:\n","        score -= (3 - found_sections_count) * 10\n","\n","    # Check for presence of clear contact information (email and phone).\n","    if re.search(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text): # Email pattern.\n","        score += 7\n","    else: # Penalize if no clear email is found.\n","        score -= 10\n","\n","    if re.search(r'(\\+\\d{1,3}[-.\\s]?)?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b', text): # Phone pattern.\n","        score += 7\n","    else: # Penalize if no clear phone number is found.\n","        score -= 10\n","\n","    # Date ranges are crucial for experience and education timelines.\n","    # Improved regex for various date range formats, including \"Present\".\n","    date_range_pattern = r'(\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)[\\s,.]*\\d{2,4}\\s*[-–to]+\\s*(?:Present|Current|Ongoing|Till Date|\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|January|February|March|April|May|June|July|August|September|October|November|December)[\\s,.]*\\d{2,4}))|\\b(\\d{1,2}/\\d{2,4}\\s*[-–to]+\\s*(?:Present|Current|Ongoing|Till Date|\\d{1,2}/\\d{2,4}))|\\b(\\d{4}\\s*[-–to]+\\s*(?:Present|Current|Ongoing|Till Date|\\d{4}))'\n","    date_ranges_found = len(re.findall(date_range_pattern, text, re.IGNORECASE))\n","    if date_ranges_found > 1: # At least two distinct periods (e.g., for different jobs/degrees).\n","        score += 10\n","    elif date_ranges_found == 1:\n","        score += 5\n","\n","    # Bullet points or clear itemization indicate good readability and structure for ATS.\n","    bullet_pattern = r'^\\s*[•·▪▫◦‣⁃*-]\\s+' # Common bullet point markers at the start of a line.\n","    bullet_points_found = len(re.findall(bullet_pattern, text, re.MULTILINE))\n","    if bullet_points_found > 3: # Several bullet points suggest well-structured descriptions.\n","        score += 10\n","    elif bullet_points_found > 0:\n","        score += 5\n","    # Alternative for structure: good use of line breaks if not using bullets.\n","    elif text.count('\\n') > (len(text) / 70): # Heuristic: sufficient newlines suggest structured items.\n","        score += 5\n","\n","    # spaCy-based analysis for structural entities (if 'nlp' is available).\n","    if 'nlp' in globals() and nlp:\n","        try:\n","            doc = nlp(text)\n","            entities = Counter([ent.label_ for ent in doc.ents])\n","\n","            # Presence of organizations (companies, schools) and dates supports structure.\n","            if entities.get('ORG', 0) > 1:\n","                score += 5\n","            if entities.get('DATE', 0) > 2 and date_ranges_found == 0 : # Add points if spaCy finds dates but regex didn't\n","                score += 5\n","\n","            # Proper nouns can indicate job titles, company names, technologies, etc.\n","            propn_count = sum(1 for token in doc if token.pos_ == 'PROPN' and not token.is_stop and len(token.text)>2)\n","            if propn_count > 8 :\n","                score += 5\n","        except Exception:\n","            # If spaCy fails, rely on regex-based checks.\n","            if re.search(r'\\b\\d{4}\\b',text_lower) and date_ranges_found == 0: score +=3\n","            pass\n","    else:\n","         if re.search(r'\\b\\d{4}\\b',text_lower) and date_ranges_found == 0: score +=3\n","\n","    return min(max(score, 0.0), 100.0) # Ensure score is capped at 100.\n","\n","\n","def _calculate_content_optimization(text: str, candidate_profile: dict) -> float:\n","    \"\"\"\n","    Scores the resume content based on the use of action verbs, quantifiable achievements (metrics),\n","    contextual skill mentions, and overall professionalism. Uses spaCy for deeper analysis.\n","    \"\"\"\n","    score = 0.0\n","    text_lower = text.lower()\n","\n","    # Expanded list of action verbs crucial for showcasing achievements.\n","    action_verbs = [\n","        'achieved', 'accelerated', 'accomplished', 'acquired', 'adapted', 'administered', 'advanced', 'advised',\n","        'advocated', 'aligned', 'analyzed', 'applied', 'approved', 'architected', 'arranged', 'assessed', 'assisted',\n","        'attained', 'audited', 'augmented', 'authored', 'automated', 'balanced', 'benchmarked', 'boosted', 'briefed',\n","        'budgeted', 'built', 'calculated', 'calibrated', 'campaigned', 'captured', 'cataloged', 'centralized',\n","        'chaired', 'championed', 'changed', 'clarified', 'classified', 'coached', 'coded', 'collaborated', 'collected',\n","        'combined', 'communicated', 'compiled', 'completed', 'composed', 'computed', 'conceived', 'conceptualized',\n","        'condensed', 'conducted', 'configured', 'consolidated', 'constructed', 'consulted', 'contacted', 'contributed',\n","        'controlled', 'converted', 'convinced', 'coordinated', 'corrected', 'counseled', 'created', 'critiqued',\n","        'cultivated', 'customized', 'debugged', 'decreased', 'defined', 'delegated', 'delivered', 'demonstrated',\n","        'deployed', 'derived', 'designed', 'detected', 'determined', 'developed', 'devised', 'diagnosed', 'directed',\n","        'discovered', 'dispatched', 'distributed', 'documented', 'doubled', 'drafted', 'drove', 'earned', 'edited',\n","        'educated', 'effected', 'elicited', 'eliminated', 'enabled', 'encouraged', 'engineered', 'enhanced',\n","        'ensured', 'entertained', 'established', 'estimated', 'evaluated', 'examined', 'exceeded', 'executed',\n","        'expanded', 'expedited', 'explained', 'explored', 'extracted', 'fabricated', 'facilitated', 'familiarized',\n","        'fashioned', 'filed', 'financed', 'focused', 'forecasted', 'formalized', 'formed', 'formulated', 'fostered',\n","        'founded', 'fulfilled', 'funded', 'gained', 'gathered', 'generated', 'governed', 'graded', 'granted',\n","        'grouped', 'grew', 'guided', 'handled', 'headed', 'helped', 'hired', 'hosted', 'identified', 'illustrated',\n","        'implemented', 'improved', 'improvised', 'inaugurated', 'increased', 'influenced', 'informed', 'initiated',\n","        'innovated', 'inspected', 'inspired', 'installed', 'instituted', 'instructed', 'insured', 'integrated',\n","        'interacted', 'interpreted', 'interviewed', 'introduced', 'invented', 'inventoried', 'invested',\n","        'investigated', 'isolated', 'issued', 'joined', 'judged', 'justified', 'keyed', 'launched', 'lectured',\n","        'led', 'licensed', 'listened', 'lobbied', 'localized', 'located', 'logged', 'lowered', 'maintained',\n","        'managed', 'manipulated', 'manufactured', 'mapped', 'marketed', 'mastered', 'maximized', 'measured',\n","        'mediated', 'mentored', 'merged', 'met', 'minimized', 'mobilized', 'modeled', 'moderated', 'modernized',\n","        'modified', 'monitored', 'motivated', 'moved', 'multiplied', 'navigated', 'negotiated', 'networked',\n","        'observed', 'obtained', 'offered', 'operated', 'optimized', 'orchestrated', 'ordered', 'organized',\n","        'oriented', 'originated', 'outlined', 'overcame', 'overhauled', 'oversaw', 'owned', 'participated',\n","        'partnered', 'patented', 'perceived', 'performed', 'persuaded', 'phased', 'piloted', 'pioneered', 'placed',\n","        'planned', 'played', 'predicted', 'prepared', 'prescribed', 'presented', 'preserved', 'presided',\n","        'prevented', 'printed', 'prioritized', 'processed', 'procured', 'produced', 'profiled', 'programmed',\n","        'projected', 'promoted', 'proofread', 'proposed', 'protected', 'proved', 'provided', 'publicized',\n","        'published', 'purchased', 'pursued', 'qualified', 'quantified', 'queried', 'questioned', 'raised', 'ranked',\n","        'rated', 'reached', 'read', 'realized', 'reasoned', 'received', 'recognized', 'recommended', 'reconciled',\n","        'recorded', 'recruited', 'rectified', 'redesigned', 'reduced', 'refined', 'refocused', 'reformatted',\n","        'regulated', 'rehabilitated', 'reinforced', 'related', 'remediated', 'remodeled', 'rendered', 'renewed',\n","        'repaired', 'replaced', 'reported', 'represented', 'researched', 'resolved', 'responded', 'restored',\n","        'restructured', 'retained', 'retrieved', 'revamped', 'reversed', 'reviewed', 'revised', 'revitalized',\n","        'rewarded', 'routed', 'ran', 'saved', 'scanned', 'scheduled', 'screened', 'scripted', 'scrutinized',\n","        'searched', 'secured', 'segmented', 'selected', 'separated', 'served', 'serviced', 'set', 'settled',\n","        'shaped', 'shared', 'shortened', 'showcased', 'simplified', 'simulated', 'sketched', 'sold', 'solved',\n","        'sorted', 'sourced', 'sparked', 'specified', 'spoke', 'sponsored', 'stabilized', 'staffed', 'staged',\n","        'standardized', 'started', 'steered', 'stimulated', 'strategized', 'streamlined', 'strengthened',\n","        'stressed', 'stretched', 'structured', 'studied', 'submitted', 'substituted', 'succeeded', 'suggested',\n","        'summarized', 'superseded', 'supervised', 'supplied', 'supported', 'surveyed', 'synthesized',\n","        'systematized', 'tabulated', 'tackled', 'tailored', 'targeted', 'taught', 'teamed', 'terminated', 'tested',\n","        'testified', 'tracked', 'traded', 'trained', 'transacted', 'transcribed', 'transferred', 'transformed',\n","        'translated', 'transmitted', 'transported', 'traveled', 'treated', 'trimmed', 'tripled', 'troubleshot',\n","        'tutored', 'uncovered', 'underlined', 'understood', 'undertook', 'unified', 'united', 'unraveled',\n","        'updated', 'upgraded', 'utilized', 'validated', 'valued', 'verbalized', 'verified', 'visualized',\n","        'volunteered', 'weighed', 'widened', 'won', 'worked', 'wrote'\n","    ]\n","\n","    action_verb_score = 0\n","    if 'nlp' in globals() and nlp:\n","        try:\n","            doc = nlp(text)\n","            # Count occurrences of lemmatized action verbs.\n","            lemmatized_verbs_in_doc = [token.lemma_.lower() for token in doc if token.pos_ == 'VERB']\n","            action_verb_found_count = sum(1 for verb_lemma in lemmatized_verbs_in_doc if verb_lemma in action_verbs)\n","\n","            # Consider verbs at the beginning of sentences or after bullets (common for impact statements).\n","            impact_action_verbs = 0\n","            for sent in doc.sents:\n","                first_token_in_sent = None\n","                for token in sent: # Find first non-space, non-punct token\n","                    if not token.is_space and not token.is_punct:\n","                        first_token_in_sent = token\n","                        break\n","                if first_token_in_sent and first_token_in_sent.pos_ == 'VERB' and first_token_in_sent.lemma_.lower() in action_verbs:\n","                    impact_action_verbs += 1\n","\n","            action_verb_score += min((action_verb_found_count * 1) + (impact_action_verbs * 1.5) , 30)\n","\n","            # Sentence complexity: Look for sentences with subordinate clauses (advcl, ccomp, xcomp, relcl).\n","            complex_sentences = sum(1 for sent in doc.sents if any(tok.dep_ in ['advcl', 'ccomp', 'xcomp', 'relcl'] for tok in sent))\n","            action_verb_score += min(complex_sentences * 1.0, 10)\n","\n","            # Professional context entities: ORG, PRODUCT, EVENT, MONEY, PERCENT, QUANTITY etc.\n","            prof_entities_count = sum(1 for ent in doc.ents if ent.label_ in ['ORG', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW', 'MONEY', 'PERCENT', 'QUANTITY', 'NORP', 'GPE'])\n","            action_verb_score += min(prof_entities_count * 0.75, 15)\n","\n","        except Exception:\n","            # Fallback to basic regex verb counting if spaCy fails.\n","            action_verb_found_count = sum(1 for verb in action_verbs if re.search(r'\\b' + re.escape(verb) + r'\\b', text_lower))\n","            action_verb_score += min(action_verb_found_count * 1.5, 30)\n","    else:\n","        action_verb_found_count = sum(1 for verb in action_verbs if re.search(r'\\b' + re.escape(verb) + r'\\b', text_lower))\n","        action_verb_score += min(action_verb_found_count * 1.5, 30)\n","    score += action_verb_score\n","\n","\n","    # Quantifiable achievements (metrics are highly valued by ATS and recruiters).\n","    # Expanded patterns to capture more types of metrics.\n","    metrics_patterns = [\n","        r'\\b\\d+(?:\\.\\d+)?%?\\s*(?:increase|decrease|improvement|reduction|growth|savings|efficiency)\\b', # 10% increase\n","        r'\\b(?:increased|decreased|improved|reduced|grew|generated|saved|optimized|achieved|exceeded|completed|managed|led|delivered)\\s(?:by\\s)?(?:over\\s|approx\\.?\\s|more than\\s|above\\s)?[\\$€£]?\\d{1,3}(?:[,.]\\d{3})*(?:\\.\\d+)?[%kKmMbB]?\\b', # increased by 10%, saved $5k\n","        r'\\b[\\$€£]\\d{1,3}(?:[,.]\\d{3})*(?:\\.\\d+)?[kKmMbB]?\\b',  # Currency values like $100k, €2.5M.\n","        r'\\b\\d+(?:\\.\\d+)?%?\\b',  # Percentages like 25%, 10.5%.\n","        r'\\b\\d+\\+?\\s*(?:years?|months?|yrs?|mos?|weeks?|days?)\\b',  # Time periods like 5+ years.\n","        r'\\b(?:over|under|above|below|approx(?:imately)?\\.?|avg\\.?|average|up to|exceeding)\\s+\\d+\\b', # Comparative numbers.\n","        r'\\b\\d+\\s*(?:projects?|clients?|users?|customers?|accounts?|downloads?|transactions?|countries|members|tasks|issues|features|products|reports|campaigns|events)\\b', # Quantities of items.\n","        r'\\b(?:from\\s[\\d\\.\\$€£%kKmMbB]+\\s(?:to|by)\\s[\\d\\.\\$€£%kKmMbB]+)\\b' # \"from X to Y\" improvements.\n","    ]\n","    metric_count = 0\n","    for pattern in metrics_patterns:\n","        metric_count += len(re.findall(pattern, text, re.IGNORECASE)) # Ignore case for metrics pattern.\n","    score += min(metric_count * 2.5, 25) # Increased weight for metrics.\n","\n","    # Skills mentioned in a contextual way (demonstrating application, not just a list).\n","    contextual_skills_score = 0\n","    if candidate_profile.get(\"candidate_skills_proficiency\"):\n","        skills_from_profile = list(candidate_profile[\"candidate_skills_proficiency\"].keys())\n","        contextual_skills_found = 0\n","\n","        if 'nlp' in globals() and nlp:\n","            try:\n","                # Assuming 'doc' is already processed from action verb analysis.\n","                # If not, process it: doc = nlp(text)\n","                for skill in skills_from_profile:\n","                    skill_lower = skill.lower()\n","                    # Use Matcher for more robust phrase matching, including multi-word skills.\n","                    # This is a simplified version; a full Matcher setup is more involved.\n","                    # For now, we'll check for the skill's presence and its syntactic context.\n","                    for token in doc:\n","                        if skill_lower in token.text.lower() or skill_lower in token.lemma_.lower():\n","                            # Check if the skill is an object of a verb, part of a prepositional phrase modifying a verb,\n","                            # or if an action verb is an ancestor.\n","                            if (token.head.pos_ == 'VERB' and token.dep_ in ['dobj', 'pobj', 'agent', 'attr']) or \\\n","                               (token.dep_ in ['pobj', 'compound'] and token.head.head.pos_ == 'VERB') or \\\n","                               any(ancestor.lemma_.lower() in action_verbs for ancestor in token.ancestors if ancestor.pos_ == 'VERB'):\n","                                contextual_skills_found += 1\n","                                break # Count skill once per contextual mention type.\n","                    # Fallback regex for simpler \"using skill\" or \"skill for X\" contexts.\n","                    else:\n","                        if re.search(rf'\\b(?:using|with|leveraging|developing|implementing|managing|leading|applying|utilizing|on|for|in)\\s+(?:the\\s+)?{re.escape(skill_lower)}\\b', text_lower) or \\\n","                           re.search(rf'\\b{re.escape(skill_lower)}\\s+(?:for|to\\s\\w+|in|on|development|implementation|analysis|design)\\b', text_lower):\n","                            contextual_skills_found +=1\n","\n","                contextual_skills_score = min(contextual_skills_found * 1.5, 20) # Max 20 points for contextual skills.\n","            except Exception:\n","                # Fallback if spaCy fails for contextual skills.\n","                contextual_skills_found = 0\n","                for skill in skills_from_profile:\n","                    if re.search(rf'\\b(?:using|with|developing|implementing|managing|leading|applying|utilizing)\\s+(?:the\\s+)?{re.escape(skill.lower())}\\b', text_lower) or \\\n","                       re.search(rf'\\b{re.escape(skill.lower())}\\s+(?:to|for|in|on|development|implementation|analysis|design)\\b', text_lower):\n","                        contextual_skills_found += 1\n","                contextual_skills_score = min(contextual_skills_found * 1.0, 15)\n","        else: # No spaCy.\n","            contextual_skills_found = 0\n","            for skill in skills_from_profile:\n","                if re.search(rf'\\b(?:using|with|developing|implementing|managing|leading|applying|utilizing)\\s+(?:the\\s+)?{re.escape(skill.lower())}\\b', text_lower) or \\\n","                   re.search(rf'\\b{re.escape(skill.lower())}\\s+(?:to|for|in|on|development|implementation|analysis|design)\\b', text_lower):\n","                    contextual_skills_found += 1\n","            contextual_skills_score = min(contextual_skills_found * 1.0, 15)\n","        score += contextual_skills_score\n","\n","    return min(max(score, 0.0), 100.0)\n","\n","\n","def _calculate_formatting_quality(text: str) -> float:\n","    \"\"\"\n","    Scores the resume based on clean, ATS-friendly formatting, and readability.\n","    Checks for appropriate length, consistent formatting, and readability metrics.\n","    \"\"\"\n","    score = 100.0 # Start high and deduct for formatting issues.\n","\n","    word_count = len(text.split())\n","    char_count = len(text)\n","    lines = text.split('\\n')\n","    non_empty_lines = len([line for line in lines if line.strip()])\n","\n","    # Resume length and density checks.\n","    if word_count < 200: # Ideal length is often debated, but very short is usually bad.\n","        score -= 25\n","    elif word_count > 1000: # Overly verbose resumes can be problematic.\n","        score -= 15\n","\n","    if word_count > 0 and (char_count / word_count) > 7.5 : # Very long words on average might indicate complex, hard-to-parse text.\n","        score -= 7\n","    if non_empty_lines > 0 and (char_count / non_empty_lines) > 100 : # Very long lines on average.\n","        score -= 10\n","\n","    # Excessive blank lines can make the resume look sparse or poorly formatted.\n","    if non_empty_lines > 0 and (len(lines) / non_empty_lines) > 2.0: # If total lines are more than double content lines.\n","        score -= 10\n","\n","    # Check for very long unbroken paragraphs (walls of text).\n","    max_consecutive_content_lines = 0\n","    current_consecutive_lines = 0\n","    for line in lines:\n","        if line.strip(): # Line has content.\n","            current_consecutive_lines +=1\n","        else: # Blank line, reset counter.\n","            max_consecutive_content_lines = max(max_consecutive_content_lines, current_consecutive_lines)\n","            current_consecutive_lines = 0\n","    max_consecutive_content_lines = max(max_consecutive_content_lines, current_consecutive_lines) # Check last block.\n","\n","    if max_consecutive_content_lines > 8 : # More than 8 consecutive content lines without a break.\n","        score -= 10\n","    elif max_consecutive_content_lines > 6:\n","        score -= 5\n","\n","    # Use of ALL CAPS (also checked in parsing, but important for formatting too).\n","    uppercase_lines = sum(1 for line in lines if line.strip() and line.strip().isupper() and len(line.strip().split()) > 1)\n","    if non_empty_lines > 0 and (uppercase_lines / non_empty_lines) > 0.15: # More than 15% of content lines are all caps.\n","        score -= 10\n","\n","    # spaCy for sentence length analysis (if 'nlp' is available).\n","    # This provides a more accurate measure of sentence structure than simple line checks.\n","    if 'nlp' in globals() and nlp:\n","        try:\n","            doc = nlp(text)\n","            sentence_token_lengths = [len([tok for tok in sent if not tok.is_punct and not tok.is_space]) for sent in doc.sents if sent.text.strip()]\n","            if sentence_token_lengths:\n","                avg_sentence_len = sum(sentence_token_lengths) / len(sentence_token_lengths)\n","\n","                if not (8 <= avg_sentence_len <= 28): # Slightly too short or too long on average.\n","                    score -= 5\n","                if avg_sentence_len > 30: # Sentences are generally too complex or verbose.\n","                    score -= 7\n","                if avg_sentence_len < 7 and len(sentence_token_lengths) > 5 : # Many very short, choppy sentences.\n","                    score -=7\n","\n","                # Check for excessive number of very long or very short sentences.\n","                num_very_long_sentences = sum(1 for s_len in sentence_token_lengths if s_len > 35)\n","                num_very_short_sentences = sum(1 for s_len in sentence_token_lengths if s_len < 6)\n","                if len(sentence_token_lengths) > 7:\n","                    if (num_very_long_sentences / len(sentence_token_lengths)) > 0.25: # >25% sentences are very long.\n","                        score -=7\n","                    if (num_very_short_sentences / len(sentence_token_lengths)) > 0.35 and avg_sentence_len < 10: # >35% sentences are very short.\n","                        score -=7\n","        except Exception:\n","            pass # No specific penalty if spaCy fails here; other checks cover general formatting.\n","\n","    # Readability score (e.g., Flesch Reading Ease) if 'textstat' library is available.\n","    # This function assumes 'flesch_reading_ease' is imported and available globally.\n","    try:\n","        if 'flesch_reading_ease' in globals() and callable(flesch_reading_ease):\n","            f_score = flesch_reading_ease(text)\n","            # Ideal Flesch score for professional documents is often cited in the 30-70 range.\n","            if 40 <= f_score <= 70:  # Good readability.\n","                score += 5 # Small bonus for good readability.\n","            elif f_score < 30:  # Very difficult to read.\n","                score -= 15\n","            elif f_score > 75: # Potentially too simplistic for some professional contexts.\n","                score -= 5\n","        else:\n","            pass # 'flesch_reading_ease' function not available.\n","    except Exception:\n","        pass # Catch any error from the flesch_reading_ease function.\n","\n","    return max(score, 0.0)\n","\n","\n","def _calculate_keyword_density(text: str, candidate_profile: dict) -> float:\n","    \"\"\"\n","    Scores the resume based on keyword optimization, considering skills from the candidate profile\n","    and general industry-relevant terms. Uses lemmatization with spaCy for better matching.\n","    \"\"\"\n","    score = 0.0\n","\n","    # Prepare text for analysis: lemmatized if spaCy is available, otherwise lowercased.\n","    text_for_analysis = text.lower()\n","    doc_tokens_for_industry_terms = [word.lower() for word in re.split(r'\\W+', text.lower()) if word and len(word)>1] # Basic tokenization for fallback.\n","\n","    if 'nlp' in globals() and nlp:\n","        try:\n","            doc = nlp(text)\n","            # Use lemmatized tokens for keyword matching.\n","            lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space]\n","            text_for_analysis = \" \".join(lemmatized_tokens) # Reconstruct text from lemmas for phrase matching.\n","            doc_tokens_for_industry_terms = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n","        except Exception:\n","            # If spaCy fails, fallback to using the original lowercased text and basic tokenization.\n","            pass\n","\n","    total_meaningful_words = len(doc_tokens_for_industry_terms)\n","    if total_meaningful_words == 0:\n","        return 0.0 # Avoid division by zero if no meaningful words are found.\n","\n","    # Skills keyword analysis based on skills provided in the candidate's profile.\n","    skills_score = 0\n","    max_possible_skills_score = 60 # Maximum points achievable from skills keywords.\n","    num_skills_evaluated = 0\n","\n","    if candidate_profile.get(\"candidate_skills_proficiency\"):\n","        skills_to_check = list(candidate_profile[\"candidate_skills_proficiency\"].keys())\n","        num_skills_evaluated = len(skills_to_check)\n","\n","        if num_skills_evaluated > 0:\n","            points_per_skill_category = { # Different point values for occurrence counts.\n","                (1, 3): 7,  # Optimal: 1-3 occurrences.\n","                (4, 4): 4,  # Slightly high: 4 occurrences.\n","                (5, float('inf')): 1 # Potential keyword stuffing: >4 occurrences.\n","            }\n","\n","            total_skill_points_achieved = 0\n","            for skill in skills_to_check:\n","                skill_to_search = skill.lower()\n","                # If spaCy was used, lemmatize the skill itself for a fair comparison.\n","                if 'nlp' in globals() and nlp and text_for_analysis != text.lower():\n","                    try:\n","                        skill_doc = nlp(skill.lower())\n","                        lemmatized_skill_tokens = [token.lemma_ for token in skill_doc if not token.is_punct and not token.is_space]\n","                        if lemmatized_skill_tokens:\n","                            skill_to_search = \" \".join(lemmatized_skill_tokens)\n","                    except Exception:\n","                        pass # Use original lowercased skill if lemmatization fails.\n","\n","                # Use regex for robust whole-word phrase matching.\n","                occurrences = len(re.findall(rf'\\b{re.escape(skill_to_search)}\\b', text_for_analysis, re.IGNORECASE))\n","\n","                for (min_occ, max_occ), points in points_per_skill_category.items():\n","                    if min_occ <= occurrences <= max_occ:\n","                        total_skill_points_achieved += points\n","                        break\n","\n","            # Normalize skill score based on the number of skills evaluated, capped by max_possible_skills_score.\n","            # Max theoretical points if all skills are optimal: num_skills_evaluated * 7.\n","            if num_skills_evaluated * 7 > 0 :\n","                 skills_score = (total_skill_points_achieved / (num_skills_evaluated * 7)) * max_possible_skills_score\n","            else:\n","                skills_score = 0\n","            skills_score = min(skills_score, max_possible_skills_score) # Ensure it doesn't exceed cap.\n","    score += skills_score\n","\n","    # Industry-relevant terms analysis (general professional keywords).\n","    # This list should be periodically updated or made domain-specific if possible.\n","    industry_terms = [\n","        'project management', 'agile methodology', 'scrum framework', 'data analysis', 'data visualization', 'business intelligence',\n","        'strategic planning', 'business development', 'market research', 'financial analysis', 'risk management', 'budgeting',\n","        'software development life cycle', 'sddlc', 'cloud computing', 'aws', 'azure', 'gcp', 'saas', 'paas', 'iaas',\n","        'machine learning', 'artificial intelligence', 'ai', 'natural language processing', 'nlp', 'deep learning',\n","        'cybersecurity', 'information security', 'network security', 'penetration testing', 'devops practices', 'ci/cd pipeline',\n","        'digital marketing', 'seo strategy', 'sem', 'content creation', 'social media marketing', 'ux/ui design principles',\n","        'customer relationship management', 'crm systems', 'salesforce', 'enterprise resource planning', 'erp systems', 'sap',\n","        'financial modeling', 'quantitative analysis', 'supply chain management', 'logistics operations', 'quality assurance', 'qa testing',\n","        'product development', 'product lifecycle', 'product roadmap', 'team leadership', 'cross-functional team', 'stakeholder engagement',\n","        'change management', 'process improvement', 'lean principles', 'six sigma', 'technical support', 'client relations',\n","        'contract negotiation', 'vendor management', 'regulatory compliance', 'data privacy', 'gdpr', 'hipaa',\n","        'communication skills', 'problem-solving', 'critical thinking', 'innovation', 'efficiency', 'optimization', 'scalability'\n","    ]\n","\n","    # If spaCy is used, lemmatize industry terms for better matching with the lemmatized resume text.\n","    processed_industry_terms = []\n","    if 'nlp' in globals() and nlp and text_for_analysis != text.lower():\n","        for term in industry_terms:\n","            try:\n","                term_doc = nlp(term) # Terms are already lowercase.\n","                lemmas = [token.lemma_ for token in term_doc if not token.is_punct and not token.is_space]\n","                if lemmas: processed_industry_terms.append(\" \".join(lemmas))\n","                else: processed_industry_terms.append(term) # Fallback if lemmatization yields nothing.\n","            except: processed_industry_terms.append(term)\n","    else:\n","        processed_industry_terms = industry_terms\n","\n","    relevant_terms_found_count = 0\n","    # Count how many of these industry terms appear in the resume.\n","    for term in processed_industry_terms:\n","        if re.search(rf'\\b{re.escape(term)}\\b', text_for_analysis, re.IGNORECASE): # Match whole words/phrases.\n","            relevant_terms_found_count +=1\n","\n","    industry_score_max = 40 # Max points from general industry terms.\n","    industry_score = min(relevant_terms_found_count * 1.0, industry_score_max) # Award points per term found, up to cap.\n","    score += industry_score\n","\n","    return min(max(score, 0.0), 100.0) # Ensure final score is capped.  n\n","# --- SWOT ANALYSIS ---\n","import math\n","\n","def generate_swot_analysis(jd_skill_importance_map: dict, candidate_profile: dict) -> dict:\n","    \"\"\"Generates a polished, sentence-embedded SWOT analysis using normalized proficiency.\"\"\"\n","    swot = {\"strengths\": [], \"weaknesses\": [], \"opportunities\": [], \"threats\": []}\n","    candidate_proficiencies = candidate_profile.get(\"candidate_skills_proficiency\", {})\n","\n","    if not candidate_proficiencies:\n","        return {k: [\"No candidate proficiency data provided.\"] for k in swot}\n","\n","    max_prof_score = max(candidate_proficiencies.values())\n","\n","    def normalize(prof_score):\n","        return (math.log1p(prof_score) / math.log1p(max_prof_score)) * 100 if max_prof_score > 0 else 0\n","\n","    strength_skills, weakness_skills, opportunity_skills, threat_skills = [], [], [], []\n","\n","    for skill, prof_score in candidate_proficiencies.items():\n","        if skill in jd_skill_importance_map:\n","            importance_score = jd_skill_importance_map[skill].get(\"score\", 0)\n","            norm_score = normalize(prof_score)\n","            if norm_score >= STRENGTH_PROFICIENCY_THRESHOLD and importance_score >= STRENGTH_IMPORTANCE_THRESHOLD:\n","                strength_skills.append(f\"{skill.capitalize()} (Proficiency: {norm_score:.0f}, JD Importance: {importance_score:.0f})\")\n","\n","    for skill, jd_data in jd_skill_importance_map.items():\n","        importance_score = jd_data.get(\"score\", 0)\n","        prof_score = candidate_proficiencies.get(skill, 0)\n","        norm_score = normalize(prof_score)\n","\n","        if importance_score >= WEAKNESS_IMPORTANCE_THRESHOLD and norm_score < WEAKNESS_PROFICIENCY_THRESHOLD:\n","            weakness_skills.append(f\"{skill.capitalize()} (Proficiency: {norm_score:.0f}, JD Importance: {importance_score:.0f})\")\n","\n","        if importance_score >= THREAT_JD_IMPORTANCE_THRESHOLD and norm_score < THREAT_PROFICIENCY_THRESHOLD:\n","            threat_skills.append(f\"{skill.capitalize()} (Proficiency: {norm_score:.0f}, JD Importance: {importance_score:.0f})\")\n","\n","    for skill, prof_score in candidate_proficiencies.items():\n","        norm_score = normalize(prof_score)\n","        if skill in jd_skill_importance_map:\n","            importance_score = jd_skill_importance_map[skill].get(\"score\", 0)\n","            if norm_score >= OPPORTUNITY_PROFICIENCY_THRESHOLD and importance_score < OPPORTUNITY_IMPORTANCE_THRESHOLD:\n","                opportunity_skills.append(f\"{skill.capitalize()} (Proficiency: {norm_score:.0f}, JD Importance: {importance_score:.0f})\")\n","        elif norm_score >= OPPORTUNITY_PROFICIENCY_THRESHOLD:\n","            opportunity_skills.append(f\"{skill.capitalize()} (Proficiency: {norm_score:.0f})\")\n","\n","    def embed_skills_in_sentence(skill_list, intro_phrase_singular, intro_phrase_plural, conclusion_phrase):\n","        if not skill_list:\n","            return \"None identified in this category based on current criteria.\"\n","        joined = \"; \".join(skill_list)\n","        if len(skill_list) == 1:\n","            return f\"{intro_phrase_singular} {joined}, {conclusion_phrase}\"\n","        else:\n","            return f\"{intro_phrase_plural} {joined}, {conclusion_phrase}\"\n","\n","    swot[\"strengths\"] = [\n","        embed_skills_in_sentence(\n","            strength_skills,\n","            \"The candidate demonstrates a key strength in\",\n","            \"The candidate demonstrates key strengths in\",\n","            \"which align strongly with the role's core requirements.\"\n","        )\n","    ]\n","\n","    swot[\"weaknesses\"] = [\n","        embed_skills_in_sentence(\n","            weakness_skills,\n","            \"A notable weakness is observed in\",\n","            \"Notable weaknesses are observed in\",\n","            \"which may impact performance in critical areas.\"\n","        )\n","    ]\n","\n","    swot[\"opportunities\"] = [\n","        embed_skills_in_sentence(\n","            opportunity_skills,\n","            \"An opportunity lies in the candidate's proficiency in\",\n","            \"Opportunities lie in the candidate's proficiencies in\",\n","            \"which can be leveraged for broader roles or future growth.\"\n","        )\n","    ]\n","\n","    swot[\"threats\"] = [\n","        embed_skills_in_sentence(\n","            threat_skills,\n","            \"A potential threat is the lack of proficiency in\",\n","            \"Potential threats arise from lack of proficiency in\",\n","            \"which could hinder the candidate’s ability to meet role expectations.\"\n","        )\n","    ]\n","\n","    return swot\n","\n","# --- SCORING AND RANKING MODULE ---\n","def calculate_candidate_raw_score(jd_skill_importance_map: dict, candidate_profile: dict) -> float:\n","    \"\"\"Calculates a raw composite score for a candidate (this is the absolute score).\"\"\"\n","    candidate_proficiencies = candidate_profile.get(\"candidate_skills_proficiency\", {})\n","    median_frequency = candidate_profile.get(\"median_frequency\", 0.0) # This is now based on JD skills\n","\n","    if not candidate_proficiencies or not jd_skill_importance_map:\n","        return 0.0\n","\n","    candidate_skill_sum = 0.0\n","    matched_skill_details = []\n","\n","    for skill, proficiency_score in candidate_proficiencies.items():\n","        if skill in jd_skill_importance_map:\n","            importance_data = jd_skill_importance_map[skill]\n","            importance_score = importance_data.get(\"score\", 0)\n","            # Product of (proficiency/100) and (importance) we are doing so as\n","            # consider someone is just 10% proficient in a skill and other is\n","            # 50% so latter will get more score in overall\n","            combined_skill_score = (proficiency_score / 100.0) * (importance_score)\n","            candidate_skill_sum += combined_skill_score\n","            if combined_skill_score > 0:\n","                 matched_skill_details.append({\n","                     \"skill\": skill, \"proficiency\": proficiency_score,\n","                     \"importance\": importance_score, \"combined\": combined_skill_score * 100\n","                 })\n","\n","    candidate_profile[\"matched_skill_details\"] = sorted(matched_skill_details, key=lambda x: x[\"combined\"], reverse=True)\n","    raw_composite_score = candidate_skill_sum\n","    return raw_composite_score\n","\n","# --- Main Execution ---\n","def main():\n","    print(\"--- Advanced Resume Analyzer and Candidate Ranker ---\")\n","\n","    # 1. Analyze Job Description\n","    print(\"\\nStep 1: Analyze Job Description\")\n","    print(\"Paste your job description below. End input with an empty line (or Ctrl+D/Ctrl+Z then Enter):\")\n","    jd_lines = []\n","    while True:\n","        try:\n","            line = input()\n","            if not line.strip() and jd_lines: break\n","            elif not line.strip() and not jd_lines: continue\n","            jd_lines.append(line)\n","        except EOFError: break\n","    job_description = \"\\n\".join(jd_lines).strip()\n","\n","    if not job_description:\n","        print(\"No job description provided. Exiting.\")\n","        return\n","\n","    jd_skill_importance_map, jd_degrees_from_jd = rate_skills_from_job_description(job_description)\n","    jd_skill_names_list = list(jd_skill_importance_map.keys()) # Get list of skill names from JD\n","\n","    print(\"\\n--- Job Description Analysis ---\")\n","    if jd_skill_importance_map:\n","        print(\"Skill Importance (0-100 scale, based on JD):\")\n","        sorted_jd_skills = sorted(jd_skill_importance_map.items(), key=lambda item: item[1][\"score\"], reverse=True)\n","        for skill, data in sorted_jd_skills:\n","            print(f\"- {skill.capitalize()} (mentioned {data['frequency']}x): {data['score']}\")\n","    else:\n","        print(\"No skills identified in the job description. Median frequency for resumes will be based on an empty skill list (resulting in 0).\")\n","\n","    print(f\"Required Degree(s) (from JD): {', '.join(jd_degrees_from_jd)}\" if jd_degrees_from_jd else \"Required Degree(s): Not clearly mentioned\")\n","\n","    print(\"\\n--- Manual JD Skill Score Adjustment (Optional) ---\")\n","    print(\"Enter skill name and new score (e.g., 'python 95'). Press Enter without input to finish.\\n\")\n","    while True:\n","        adjustment = input(\"Enter adjustment (or press Enter to skip/finish): \").strip()\n","        if not adjustment: break\n","        parts = adjustment.split()\n","        try:\n","            new_score = int(parts[-1])\n","            skill_name_to_adjust = \" \".join(parts[:-1]).lower()\n","            if skill_name_to_adjust in jd_skill_importance_map:\n","                old_score = jd_skill_importance_map[skill_name_to_adjust]['score']\n","                jd_skill_importance_map[skill_name_to_adjust]['score'] = new_score\n","                print(f\"Updated '{skill_name_to_adjust}' score from {old_score} to {new_score}\")\n","            else:\n","                jd_skill_importance_map[skill_name_to_adjust] = {'score': new_score, 'frequency': 1}\n","                print(f\"Added '{skill_name_to_adjust}' with score {new_score} (frequency set to 1)\")\n","            # Update jd_skill_names_list if a new skill was added or if needed (though keys() will reflect it)\n","            jd_skill_names_list = list(jd_skill_importance_map.keys())\n","        except (IndexError, ValueError): print(\"Invalid format. Use 'skill_name new_score'.\")\n","\n","    if jd_skill_importance_map: generate_jd_treemap(jd_skill_importance_map)\n","\n","    # Calculate maximum possible absolute score for percentage calculation\n","    max_absolute_score = calculate_max_possible_absolute_score(jd_skill_importance_map)\n","\n","\n","    # 2. Analyze Resumes\n","    print(\"\\nStep 2: Analyze Resumes\")\n","    resume_folder_path = input(\"Enter the path to the folder containing resume PDF files: \").strip()\n","    if not os.path.isdir(resume_folder_path):\n","        print(\"Invalid folder path. Exiting.\")\n","        return\n","\n","    all_candidate_profiles = []\n","    pdf_files = [f for f in os.listdir(resume_folder_path) if f.lower().endswith(\".pdf\")]\n","    if not pdf_files:\n","        print(f\"No PDF files found in '{resume_folder_path}'. Exiting.\")\n","        return\n","    print(f\"Found {len(pdf_files)} PDF files. Processing up to 100.\")\n","\n","    for filename in pdf_files[:100]:\n","        full_path = os.path.join(resume_folder_path, filename)\n","        # Pass jd_skill_names_list to analyze_resume_for_candidate\n","        candidate_profile = analyze_resume_for_candidate(full_path, jd_skill_names_list)\n","        if candidate_profile:\n","            if candidate_profile.get(\"error\"):\n","                print(f\"  Skipping scoring for {filename} due to error: {candidate_profile['error']}\")\n","\n","            is_qualified, edu_message = check_educational_qualification(jd_degrees_from_jd, candidate_profile.get(\"degrees\", []))\n","            candidate_profile[\"education_qualified\"] = is_qualified\n","            candidate_profile[\"education_message\"] = edu_message\n","            print(f\"  Education Check: {edu_message}\")\n","\n","            all_candidate_profiles.append(candidate_profile)\n","\n","    if not all_candidate_profiles:\n","        print(\"No resume data could be processed. Exiting.\")\n","        return\n","\n","    # 3. Score and Rank Candidates\n","    print(\"\\nStep 3: Score and Rank Candidates\")\n","    candidate_raw_scores_data = []\n","    for profile in all_candidate_profiles:\n","        absolute_raw_score = 0.0 # Default for error cases\n","        ats_score = 0.0 # Default ATS score\n","        resume_text = extract_text_from_pdf(os.path.join(resume_folder_path, profile[\"filename\"])) if not profile.get(\"error\") else \"\"\n","        if profile.get(\"error\"):\n","            absolute_raw_score = -1 # Mark as error for score normalization\n","            ats_score = 0.0 # ATS score is 0 for error cases\n","        else:\n","            # This calculates the absolute score\n","            absolute_raw_score = calculate_candidate_raw_score(jd_skill_importance_map, profile)\n","            # Calculate ATS score (independent of JD)\n","            ats_score = calculate_ats_score(profile, resume_text, os.path.join(resume_folder_path, profile[\"filename\"]))\n","\n","        candidate_raw_scores_data.append({\n","            \"filename\": profile[\"filename\"],\n","            \"absolute_raw_score\": absolute_raw_score, # Store the absolute score\n","            \"ats_score\": ats_score, # Store the ATS score\n","            \"profile_details\": profile\n","        })\n","\n","    # Calculate absolute score percentages and normalize\n","    valid_abs_scores = [item[\"absolute_raw_score\"] for item in candidate_raw_scores_data if item[\"absolute_raw_score\"] >= 0]\n","    max_overall_abs_score = max(valid_abs_scores) if valid_abs_scores else 0\n","\n","    final_ranked_candidates = []\n","    for item in candidate_raw_scores_data:\n","        normalized_relative_score = 0.0\n","        current_abs_score = item[\"absolute_raw_score\"]\n","\n","        # Calculate absolute score percentage using logarithmic scaling\n","        abs_score_percentage = (np.log1p(current_abs_score) / np.log1p(max_overall_abs_score)) * 100 if current_abs_score >= 0 and max_overall_abs_score > 0 else 0\n","\n","        # Normalize based on absolute_score_percentage\n","        if current_abs_score < 0: # Error case\n","            normalized_relative_score = 0.0\n","        else:\n","            # Normalize using abs_score_percentage\n","            max_abs_score_percentage = (np.log1p(max_overall_abs_score) / np.log1p(max_overall_abs_score)) * 100 if max_overall_abs_score > 0 else 0\n","            normalized_relative_score = round((abs_score_percentage / max_abs_score_percentage) * 100, 2) if max_abs_score_percentage > 0 else 0.0\n","\n","        final_ranked_candidates.append(\n","            (item[\"filename\"], current_abs_score, normalized_relative_score, item[\"ats_score\"], item[\"profile_details\"])\n","        )\n","\n","    # Sort by normalized_relative_score (index 2 of the tuple)\n","    final_ranked_candidates.sort(key=lambda x: x[2], reverse=True)\n","\n","    print(\"\\n--- Final Candidate Ranking ---\")\n","    qualified_candidates = []\n","    unqualified_candidates = []\n","\n","    # Separate qualified and unqualified candidates\n","    for candidate in final_ranked_candidates:\n","        if not candidate[4].get(\"education_qualified\", True):\n","            unqualified_candidates.append(candidate)\n","        else:\n","            qualified_candidates.append(candidate)\n","\n","    # Print qualified candidates\n","    for i, (filename, abs_score, norm_score, ats_score, profile) in enumerate(qualified_candidates):\n","        # Display absolute score as a percentage (abs_score / max_absolute_score)\n","        abs_score_percentage = (np.log1p(abs_score) / np.log1p(max_absolute_score)) * 100 if abs_score >= 0 and max_absolute_score > 0 else 0\n","        abs_score_display = f\"{abs_score_percentage:.2f}\"\n","\n","        print(f\"\\n{i+1}. {filename}: Absolute Score(out of 100): {abs_score_display}, Normalized Score(out of 100): {norm_score:.2f}, ATS Score(out of 100): {ats_score:.2f}\")\n","\n","        if profile.get(\"error\"):\n","            print(f\"     Status: Error during processing - {profile.get('error')}\")\n","            continue\n","\n","        if \"matched_skill_details\" in profile and profile[\"matched_skill_details\"]:\n","            top_matches = profile[\"matched_skill_details\"][:3]\n","            match_strs = [m['skill'] for m in top_matches]\n","            print(f\"     Top Matched Skills: {'; '.join(match_strs)}\")\n","        print(f\"     Degrees: {', '.join(profile['degrees']) if profile['degrees'] else 'N/A'}\")\n","\n","        if jd_skill_importance_map:\n","            swot = generate_swot_analysis(jd_skill_importance_map, profile)\n","            print(f\"     Debug: Generating SWOT for {filename}\")\n","            print(\"     SWOT Analysis:\")\n","            print(f\"       Strengths: {'; '.join(swot['strengths'])}\")\n","            print(f\"       Weaknesses: {'; '.join(swot['weaknesses'])}\")\n","            print(f\"       Opportunities: {'; '.join(swot['opportunities'])}\")\n","            print(f\"       Threats: {'; '.join(swot['threats'])}\")\n","        else:\n","            print(\"     SWOT Analysis: Skipped (No JD skill importance data available).\")\n","\n","    # Print unqualified candidates\n","    if unqualified_candidates:\n","        print(\"\\n--- Resumes Not Meeting Minimum Education Criteria ---\")\n","        for i, (filename, abs_score, norm_score, ats_score, profile) in enumerate(unqualified_candidates):\n","            # Display absolute score as a percentage (abs_score / max_absolute_score)\n","            abs_score_percentage = (np.log1p(abs_score) / np.log1p(max_absolute_score)) * 100 if abs_score >= 0 and max_absolute_score > 0 else 0\n","            abs_score_display = f\"{abs_score_percentage:.2f}\"\n","            edu_status_msg = f\" (WARNING: {profile.get('education_message', 'Does not meet educational requirements.')})\"\n","\n","            print(f\"\\n{i+1}. {filename}: Absolute Score(out of 100): {abs_score_display}, Normalized Score(out of 100): {norm_score:.2f}, ATS Score(out of 100): {ats_score:.2f}{edu_status_msg}\")\n","\n","            if profile.get(\"error\"):\n","                print(f\"     Status: Error during processing - {profile.get('error')}\")\n","                continue\n","\n","            if \"matched_skill_details\" in profile and profile[\"matched_skill_details\"]:\n","                top_matches = profile[\"matched_skill_details\"][:3]\n","                match_strs = [m['skill'] for m in top_matches]\n","                print(f\"     Top Matched Skills: {'; '.join(match_strs)}\")\n","            print(f\"     Degrees: {', '.join(profile['degrees']) if profile['degrees'] else 'N/A'}\")\n","\n","            if jd_skill_importance_map:\n","                swot = generate_swot_analysis(jd_skill_importance_map, profile)\n","                print(f\"     Debug: Generating SWOT for {filename}\")\n","                print(\"     SWOT Analysis:\")\n","                print(f\"       Strengths: {'; '.join(swot['strengths'])}\")\n","                print(f\"       Weaknesses: {'; '.join(swot['weaknesses'])}\")\n","                print(f\"       Opportunities: {'; '.join(swot['opportunities'])}\")\n","                print(f\"       Threats: {'; '.join(swot['threats'])}\")\n","            else:\n","                print(\"     SWOT Analysis: Skipped (No JD skill importance data available).\")\n","\n","\n","\n","    print(\"\\n--- Analysis Complete ---\")\n","\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-R6WzcfRaJ6A"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.2"}},"nbformat":4,"nbformat_minor":0}